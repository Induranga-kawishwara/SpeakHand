{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a9c6d4-d11e-471d-b2d3-a1f62c758b0d",
   "metadata": {},
   "source": [
    "# Import and install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e57959-237f-427f-9a2b-91887e6f9e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6543ab32-7093-46e8-9efd-e558263a91bf",
   "metadata": {},
   "source": [
    "# Create Keypoints using Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411863a8-aada-4cf9-a202-0f7499bca24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf7ea1d-ca89-4e7a-b640-e42a7995dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd87140f-bca9-49d5-9770-d097f1b35da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7b8e3-d95a-4b4b-8ab1-d16c2598e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8adac5-3c49-4063-bddf-35d24e26740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        #  # Flip the frame horizontally\n",
    "        # frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "         # Resize and show the image\n",
    "        image = cv2.resize(image, (1000,800))\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('SignPal', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a89af-0273-4383-9778-409dab0f1428",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecda4b8-9835-43f8-9312-606aeec2ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(cv2.flip(image,1), cv2.COLOR_BGR2RGB)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0600a-c3f0-41b2-b0cf-346628f19d37",
   "metadata": {},
   "source": [
    "# Extract keypoint values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00455bee-591c-464c-8476-5947579cc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ce74c-cbf7-4c05-9234-60f54a7796a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd7230-1d19-4932-90a4-a8d8b0ed584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc73e42-3e13-4a19-959b-faa27d2d5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b113c-db35-4401-87ff-970f321f68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e9d4fb-e164-49f0-b9b2-4db15b669c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930eeffc-f62c-4e12-a539-b666b57fd5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('0', result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de1c00-759c-4a07-9023-31cbbe58b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('0.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eee1140-8a69-49e5-a821-75777d481f30",
   "metadata": {},
   "source": [
    "# Setup Folders for Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f1548-11f0-478e-8a72-1280deb3b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "# actions = np.array(['1','2','3','4','5','6','7','8','9','10','11','12','13','14','A','Afternoon','Ayubowan','B','C','D','E','Evening','F','Friday','G','Good','H','I','J','Monday', 'Morning', 'Night','Noon','Saturday','Sunday','Thursday','Tuesday','Wednesday'])\n",
    "actions = np.array(['1','2','3','Afternoon','Ayubowan','Evening','Good']) \n",
    "\n",
    "# Save the labels to a text file\n",
    "with open('dataset_label.txt', 'w') as f:\n",
    "    for label in actions:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# # Ensure the directory structure exists\n",
    "# for action in actions:\n",
    "#     action_path = os.path.join(DATA_PATH, action)\n",
    "#     if not os.path.exists(action_path):\n",
    "#         os.makedirs(action_path)  # Create folder if it does not exist\n",
    "#         print(f\"Created folder: {action_path}\")\n",
    "\n",
    "# Folder start\n",
    "# start_folder = 0\n",
    "# for action in actions: \n",
    "#     dirmax = np.max(np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int)) if os.listdir(os.path.join(DATA_PATH, action)) else 0\n",
    "#     for sequence in range(1, no_sequences + 1):\n",
    "#         try: \n",
    "#             os.makedirs(os.path.join(DATA_PATH, action, str(dirmax + sequence)))\n",
    "#         except:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d55d3-a6ae-4d8b-aa08-9ae2421926d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    # dirmax = np.max(np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int))\n",
    "    for sequence in range(no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass\n",
    "# for action in actions:\n",
    "#     action_path = os.path.join(DATA_PATH, action)\n",
    "#     if not os.path.exists(action_path):\n",
    "#         os.makedirs(action_path)\n",
    "#     dirmax = np.max(np.array(os.listdir(action_path)).astype(int)) if os.listdir(action_path) else 0\n",
    "#     for sequence in range(1, no_sequences + 1):\n",
    "#         try:\n",
    "#             os.makedirs(os.path.join(action_path, str(dirmax + sequence)))\n",
    "#         except:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa67c7f-3244-4d01-9b4d-075546e66c4a",
   "metadata": {},
   "source": [
    "# Collect keypoint values for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b2077-0209-4db2-849c-3de2fb169c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,20), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX,0.8, (0, 225, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                    # Resize and show the image\n",
    "                    image = cv2.resize(image, (1000,800))\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('SignPal', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,20), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 225, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                     # Resize and show the image\n",
    "                    image = cv2.resize(image, (1000,800))\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('SignPal', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                # # Ensure directory exists\n",
    "                # os.makedirs(os.path.dirname(npy_path), exist_ok=True)\n",
    "                \n",
    "                # Save keypoints\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7c389-9752-498a-9389-9ee2a9364b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce4074f-fbb7-4dc7-bb8a-fd9a5a62c099",
   "metadata": {},
   "source": [
    "# Processing and data labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe5bf7b-007a-4280-969e-9efca67dd9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e668415c-57e1-46f6-88ee-5ca5a1e84d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939e3d56-68f4-4915-9da2-daeb75d7ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c51b8e9-88e9-4ec5-96fd-03931e9c3a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences, labels = [], []\n",
    "# for action in actions:\n",
    "#     for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "#         window = []\n",
    "#         for frame_num in range(sequence_length):\n",
    "#             res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "#             window.append(res)\n",
    "#         sequences.append(window)\n",
    "#         labels.append(label_map[action])\n",
    "sequences, labels = [], [] \n",
    "for action in actions: \n",
    "    for sequence in range(no_sequences):\n",
    "        window = []  # This line should be properly aligned with the inner for-loop\n",
    "        for frame_num in range(sequence_length): \n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8914339-b38f-4a61-8425-f77d70307b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec20e72-ad06-4426-a72e-e60aec19be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d496d-b67b-4e49-945b-185b4e3ab999",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f464fcf-e69a-4ca0-8e1c-a70393d2330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76449742-3aeb-472e-90b1-a4955e5aacd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e93132-f535-46aa-80b9-cd46ec0722d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42, stratify=y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Assuming X_data and y_data are your features and labels\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# np.save(\"X_test.npy\", X_test)\n",
    "# np.save(\"y_test.npy\", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a04f6e7-e91d-4db8-8e45-20879d06c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29554eff-2001-4ca6-b1a2-b87cf766f8bb",
   "metadata": {},
   "source": [
    "# Build and train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf97eba-20e5-4f42-894a-0d9cce3e28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d235ff-02ba-4678-a2e0-d2aa3849009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907e6bdb-725a-468c-80e8-787a01c4cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# # Increase LSTM units and add more layers\n",
    "# model.add(LSTM(128, return_sequences=True, activation='relu', input_shape=(30, 1662)))\n",
    "# model.add(Dropout(0.2))  # Prevent overfitting\n",
    "# model.add(BatchNormalization())  # Normalize activations\n",
    "\n",
    "# model.add(LSTM(256, return_sequences=True, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(LSTM(128, return_sequences=False, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# # Increase Dense layer size\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# # Output layer with softmax\n",
    "# model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3989c633-030c-4d4e-80e9-f51761a46fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d232697-a369-4c57-b253-99baa8d8184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4cbf60-7c93-4e9a-8435-3b5821b22d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e7b66-76d6-41cc-bc3a-7387e32a0b42",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28857554-67bc-4dc0-aab9-2a58c6eeeaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d56ac7d-bcd3-4122-8cdd-e2ff0821d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabaf3a7-8b37-40ec-b662-967e1ee36bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b675b12-5003-406c-8cfd-ed1be68da88a",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd34192-0bfb-4ab3-b45c-aeb1e35dadec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5cf87-01c5-40ff-bd41-5ebc21faa302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in .h5 format\n",
    "model.save('my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f90b1e-f996-4433-b3f8-30a00a8da872",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bdfd6d-71c0-4781-997c-e85b1ca5b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "model = load_model('my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70fefe2-48ee-4fd6-824e-b74a1f9f53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb82105-3a0d-4d2b-b909-91c4b499e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "# Load the Keras model from the .h5 file\n",
    "model = tf.keras.models.load_model('my_model.keras')\n",
    "\n",
    "# Convert the model to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# Enable resource variables\n",
    "converter.experimental_enable_resource_variables = True\n",
    "\n",
    "# Use select TensorFlow ops\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "\n",
    "# Disable lowering of tensor list ops\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted .tflite model to a file\n",
    "with open('action_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Save the .tflite model in .pkl format\n",
    "with open('action_model.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(tflite_model, pkl_file)\n",
    "\n",
    "print(\"Model saved in .tflite and .pkl formats!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e5645-21ff-4c5d-8944-d2352e021a41",
   "metadata": {},
   "source": [
    "# conusion matrix,accuracy, classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4845b161-bcbd-42c1-bd35-8422ebb09bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate Model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Convert softmax outputs to class labels\n",
    "y_true_classes = np.argmax(y_test, axis=1)  # Convert one-hot labels to class labels\n",
    "\n",
    "# Compute Accuracy, Precision, Recall, F1 Score\n",
    "print(\"Classification Report:\\n\", classification_report(y_true_classes, y_pred_classes))\n",
    "\n",
    "# Compute Confusion Matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=actions, yticklabels=actions)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy & Loss Graphs\n",
    "history = model.history.history\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy Graph\n",
    "plt.subplot(1, 2, 1)\n",
    "if 'categorical_accuracy' in history:\n",
    "    plt.plot(history['categorical_accuracy'], label='Train Accuracy')\n",
    "else:\n",
    "    print(\"Train Accuracy not found in history.\")\n",
    "\n",
    "if 'val_categorical_accuracy' in history:\n",
    "    plt.plot(history['val_categorical_accuracy'], label='Validation Accuracy')\n",
    "else:\n",
    "    print(\"Validation Accuracy not found in history.\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Graph')\n",
    "plt.legend()\n",
    "\n",
    "# Loss Graph\n",
    "plt.subplot(1, 2, 2)\n",
    "if 'loss' in history:\n",
    "    plt.plot(history['loss'], label='Train Loss')\n",
    "else:\n",
    "    print(\"Train Loss not found in history.\")\n",
    "\n",
    "if 'val_loss' in history:\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "else:\n",
    "    print(\"Validation Loss not found in history.\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Graph')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve for Multi-class\n",
    "plt.figure(figsize=(10, 7))\n",
    "for i in range(y_test.shape[1]):  # Iterate through each class\n",
    "    fpr, tpr, _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'Class {actions[i]} (AUC = {auc_score:.2f})')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead3b8fe-fcc3-4c00-a7a9-829ff634f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b522be3-5b1a-44be-bb90-0eaaa1d150df",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7785c-96ff-42a3-b33e-3fcd659cc9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff9903-375d-4a64-b92b-432ebe281516",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83d07d-4f0b-4014-b3d2-e876ac6e26bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ce9ad-c6e5-47f7-b2b7-7953e7fdbec2",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3bd7f9-b418-4483-9496-b21f5ecaa14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952ea61a-30f8-47c6-ac6b-0a67b9bf9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty sentence to store detected actions\n",
    "sentence = []\n",
    "previous_action = None\n",
    "\n",
    "# colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "# if len(colors) < len(actions):\n",
    "#     colors = [tuple(np.random.randint(0, 255, 3).tolist()) for _ in range(len(actions))]\n",
    "# def prob_viz(res, actions, input_frame, colors):\n",
    "#     output_frame = input_frame.copy()\n",
    "#     for num, prob in enumerate(res):\n",
    "#         cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "#         cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "#     return output_frame\n",
    "\n",
    "\n",
    "# Function to visualize the most likely action and form sentences\n",
    "def prob_viz(res, actions, input_frame):\n",
    "    global sentence, previous_action\n",
    "    output_frame = input_frame.copy()\n",
    "\n",
    "    # Get the index of the highest probability\n",
    "    max_prob_index = np.argmax(res)\n",
    "\n",
    "    # Define the action with the highest probability\n",
    "    action = actions[max_prob_index]\n",
    "\n",
    "    # Append the action to the sentence only if it's not repeated\n",
    "    if action != previous_action:\n",
    "        sentence.append(action)\n",
    "        previous_action = action\n",
    "\n",
    "    # Limit the length of the sentence to avoid overflowing\n",
    "    if len(sentence) > 5:  # Change 10 to any number you prefer for sentence length\n",
    "        sentence = sentence[-5:]\n",
    "\n",
    "    # # Display the sentence at the bottom of the frame\n",
    "    # cv2.putText(output_frame, ' '.join(sentence), \n",
    "    #             (10, output_frame.shape[0] - 20),  # Position it horizontally on the left\n",
    "    #             cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    ## Join the sentence to form the text to display\n",
    "    text = ' '.join(sentence)\n",
    "\n",
    "# Get the text size\n",
    "    (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "\n",
    "# Calculate the x-coordinate for center alignment\n",
    "    x = (output_frame.shape[1] - text_width) // 2\n",
    "\n",
    "# y-coordinate remains near the bottom\n",
    "    y = output_frame.shape[0] - 20\n",
    "\n",
    "# Display the text at the bottom center of the frame\n",
    "    cv2.putText(output_frame, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85edce-122c-4003-8ad6-7977b6d6c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(prob_viz(res, actions, image))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd6e03-3f3b-49f5-b20b-d1289a1240b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize variables for sentence formation and predictions\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "# Function to visualize the most likely action and form sentences\n",
    "def prob_viz(res, actions, input_frame):\n",
    "    global sentence\n",
    "    output_frame = input_frame.copy()\n",
    "    return output_frame\n",
    "\n",
    "# Start capturing video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Load the mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        # Read frame from video capture\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections using mediapipe\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        # Draw landmarks on the detected frame\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # Extract keypoints from the results\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]  # Keep the last 30 frames\n",
    "        \n",
    "        # Prediction logic (Make prediction once sequence has 30 frames)\n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            # Check if the prediction is consistent over the last 10 frames\n",
    "            if np.unique(predictions[-10:])[0] == np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold:  # Apply the threshold\n",
    "\n",
    "                    # Only append the action if it's different from the last one\n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            # Keep the sentence length limited to the last 5 words\n",
    "            if len(sentence) > 1: \n",
    "                sentence = sentence[-1:]\n",
    "\n",
    "        # Visualization\n",
    "        if len(sequence) == 30:\n",
    "            image = prob_viz(res, actions, image)\n",
    "        \n",
    "        # Draw a rectangle at the bottom of the image to display the sentence\n",
    "        height, width, _ = image.shape\n",
    "        text = ' '.join(sentence)\n",
    "        (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        x = (width - text_width) // 2\n",
    "        y = height - 10\n",
    "        # Display the text at the bottom center of the frame\n",
    "        cv2.putText(image, text, (x, y), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Resize and show the image\n",
    "        image = cv2.resize(image, (1000,800))\n",
    "        \n",
    "        # Display the output image\n",
    "        cv2.imshow('SignPal', image)\n",
    "\n",
    "        # Break the loop when 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the capture and close any open windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
